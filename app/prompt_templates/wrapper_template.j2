import asyncio
import requests
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import random
import time
from utils import RateLimitHandler, mask_credentials, HistoricalDataFetcher, MessageQueueSender, get_interval_from_periodicity, ContinuousExecutor
{{ additional_imports }}

class ATTWrapper:
    def __init__(self, wrapper_id: str, source_config: dict = None, rabbitmq_url: str = None):
        # CRITICAL: wrapper_id is always a STRING (UUID), never an integer
        self.wrapper_id = wrapper_id
        self.rabbitmq_url = rabbitmq_url or os.getenv('AMQP_URL', 'amqp://guest:guest@localhost/')
        self.source_type = "{{ source_type }}"
        self.periodicity = "{{ periodicity }}"
        self.rate_limiter = RateLimitHandler()
        self.historical_data_fetched = False
        self.source_config = source_config or {}
        
        # Build auth headers from source_config
        self.headers = dict(self.source_config.get('custom_headers', {}))
        auth_type = self.source_config.get('auth_type', 'none')
        if auth_type == 'api_key' and self.source_config.get('api_key'):
            header_name = self.source_config.get('api_key_header', 'X-API-Key')
            self.headers[header_name] = self.source_config['api_key']
        elif auth_type == 'bearer' and self.source_config.get('bearer_token'):
            self.headers['Authorization'] = f"Bearer {self.source_config['bearer_token']}"
        elif auth_type == 'basic' and self.source_config.get('username'):
            import base64
            credentials = base64.b64encode(f"{self.source_config['username']}:{self.source_config.get('password', '')}".encode()).decode()
            self.headers['Authorization'] = f"Basic {credentials}"
        
        # Wrapper metadata
        self.metadata = {
            "source_type": self.source_type,
            "periodicity": self.periodicity,
            "wrapper_version": "1.0"
        }
        
        # Initialize utilities
        self.message_sender = MessageQueueSender(self.wrapper_id, self.rabbitmq_url, self.metadata)
        self.historical_fetcher = HistoricalDataFetcher(self.wrapper_id, self.get_interval_seconds)
        self.continuous_executor = ContinuousExecutor(self.wrapper_id, self.get_interval_seconds())
        
    def get_interval_seconds(self) -> int:
        """Convert periodicity to seconds for API sources"""
        return get_interval_from_periodicity(self.periodicity)
    
    async def fetch_historical_data(self):
        """
        Fetch historical data from API with date range parameters.
        Returns HistoricalFetchResult with water marks for resumability.

        *** DO NOT MODIFY THIS METHOD ***
        This method uses the HistoricalDataFetcher utility for optimal batch processing.
        Only customize fetch_date_range() and fetch_external_data() methods.
        """
        print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Starting historical data fetch...")
        result = await self.historical_fetcher.fetch_all_historical_data(
            fetch_date_range_func=self.fetch_date_range,
            send_to_queue_func=self.send_to_queue
        )
        print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Completed historical data fetch - {result.total_points_sent} data points sent")
        return result
    
    async def fetch_date_range(self, start_date: datetime, end_date: datetime) -> List[Dict[str, Any]]:
        """Fetch data for a specific date range"""
        # PLACEHOLDER: Implement date range fetching
        # Use tools to discover which date query parameters the API accepts (e.g. from/to, start/end, startDate/endDate)
        # Only use the parameters that actually work â€” do NOT send multiple parameter families simultaneously
        
        print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Fetching historical data from {start_date} to {end_date}...")
        
        # IMPORTANT: Always use self.headers for authentication in all requests
        # response = requests.get(url, headers=self.headers, params=params)
        # await self.rate_limiter.handle_rate_limit(response)
        
        ...
        return []

    async def fetch_external_data(self) -> List[Dict[str, Any]]:
        """
        Fetch data from external source and convert to timeseries format
        [{'x': 'YYYY-MM-DD HH:MM:SS', 'y': VALUE}, ...]
        """
        # PLACEHOLDER: Replace this with actual data fetching logic

        print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Starting current data fetch from {self.source_type} source...")

        try:
            # PLACEHOLDER FOR SOURCE-SPECIFIC IMPLEMENTATION
            ...

            # Mock data for demonstration - replace with actual logic
            base_time = datetime.now()
            data_points = []

            for i in range(10):
                point_time = base_time + timedelta(minutes=i * 5)
                data_points.append({
                    "x": point_time.isoformat(),
                    "y": random.uniform(0, 100)
                })

            print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Successfully fetched {len(data_points)} data points")
            return data_points

        except (ValueError, KeyError, TypeError, ConnectionError, TimeoutError, OSError) as e:
            print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Error fetching external data: {str(e)}")
            raise
    
    async def send_to_queue(self, data_points: List[Dict[str, Any]]):
        """Send formatted data to RabbitMQ queue"""
        print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Publishing {len(data_points)} data points to queue...")
        print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Message content: {data_points}")
        await self.message_sender.send_to_queue(data_points)
        print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Successfully published message to queue")
    
    async def run_once(self):
        """Execute one cycle of data fetching and sending"""
        print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Starting data fetch cycle for {self.source_type} source")
        data_points = await self.fetch_external_data()
        if data_points:
            await self.send_to_queue(data_points)
            print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Completed data fetch cycle successfully")
        else:
            print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: No data to send")
            raise Exception("No data points were fetched from the source")
    
    async def run_continuous(self):
        """Run the wrapper continuously with intervals based on periodicity"""
        interval_seconds = self.get_interval_seconds()
        print(f"[{datetime.now().isoformat()}] Wrapper {self.wrapper_id}: Starting continuous execution with {self.periodicity} periodicity ({interval_seconds} seconds)")
        await self.continuous_executor.run_continuous(
            run_once_func=self.run_once,
            fetch_historical_func=self.fetch_historical_data if not self.historical_data_fetched else None,
            fetch_date_range_func=self.fetch_date_range if self.source_type == "API" else None,
            send_to_queue_func=self.send_to_queue if self.source_type == "API" else None,
            source_type=self.source_type,
            on_phase_change=lambda phase: setattr(self.message_sender, 'current_phase', phase),
        )
    
# Entry point for generated wrapper
if __name__ == "__main__":
    import sys

    print(f"[{datetime.now().isoformat()}] Wrapper starting with args: {sys.argv}")

    if len(sys.argv) < 3:
        raise ValueError("wrapper_id and source_config are required as arguments")
    wrapper_id = sys.argv[1]
    source_config_json = sys.argv[2]

    # Parse source config
    import json
    try:
        source_config = json.loads(source_config_json)
    except json.JSONDecodeError:
        raise ValueError("source_config must be valid JSON")

    wrapper = ATTWrapper(wrapper_id, source_config=source_config)

    # Check for skip-historical flag (for service restarts)
    if len(sys.argv) > 3 and sys.argv[3] == "--skip-historical":
        wrapper.historical_data_fetched = True
        print(f"[{datetime.now().isoformat()}] Wrapper {wrapper_id}: Skipping historical data fetch (restart mode)")

    # Determine execution mode based on source type
    if wrapper.source_type in ["CSV", "XLSX"]:
        print(f"File source ({wrapper.source_type}) detected - running once")
        asyncio.run(wrapper.run_once())
    else:
        print(f"API source detected - running continuously with {wrapper.periodicity} periodicity")
        asyncio.run(wrapper.run_continuous())